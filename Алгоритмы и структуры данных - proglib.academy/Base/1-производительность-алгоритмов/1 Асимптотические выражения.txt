Анализ алгоритма:
    Часто исходные данные характеризуются натуральным числом n (прим. [N1 ... Nn])
    Тогда время работы алгоритма – T(n)
    Объем доп. памяти – M(n).
Пример. Сортировка массива. Важен размер исходного массива – n.



О большое
O(g(n)) = {f(n): существуют положительные константы c и n0, такие что для любого n ≥ n0 выполняется 0 ≤ f(n) ≤ c*g(n)}
O(функции g(·)) = множество {функций f(·) таких, что …}

Примеры:
f(n) = n
f(n) ∈ O(n^2) ? 
иначе пишут f(n) = O(n^2)
Да, т.к. n ≤ n^2 для всех n ≥ 1
n это f, n^2 это g, 1 это c

5 * n = O(n) ?
существует ли такая c, при которой 5 * n ≤ c * n
Да, 6

25 * sqrt(n) = O(n)
25 * sqrt(n) ≤ 26 * n, n ≥ 1



T(n) = O(n) – Линейное время работы
T(n) = O(n^2) – Квадратичное
T(n) = O(log n)
T(n) = O(n log n)
T(n) = O(e^n) - экспоненциальное



Другие классы
Можно ограничивать функции не только сверху, но и снизу.
Для этого используются аналогичные классы Ω и Θ

Θ (тэта) - функция ограничена сверху и снизу
Ω (омега)- функция ограничена снизу

Используются чтобы сказать: 
    алгоритм работает всегда НЕ МЕНЕЕ ЧЕМ, за такое-то время
    алгоритм работает НЕ ДОЛЬШЕ ЧЕМ, НЕ ТРАТИТ памяти не больше чем
    пример для низа: сортировка СРАВНЕНИЯМИ в худшем случае будет работать НЕ БЫСТРЕЕ чем O(n log n)



Итог:
    f(n)    - наша функция    
    c*g(n)  - O(функция g(n))
    утверждения снизу верны для n ≥ n0
    
    O(g(n)) - ограничение сверху, 0 ≤ f(n) ≤ c*g(n)
    Ω(g(n)) - ограничение снизу, 0 ≤ c*g(n) ≤ f(n)
    Θ(g(n)) - ограничение СВЕРХУ и СНИЗУ,  0 ≤ c1*g(n) ≤ f(n) ≤ c2*g(n)
